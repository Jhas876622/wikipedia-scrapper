# ğŸŒ Wikipedia Web Scraper

A Python-based project that scrapes structured information from **Wikipedia pages**, such as titles, infobox content, tables, paragraphs, and links. Built using `Requests`, `BeautifulSoup`, and `Pandas`, this project is ideal for learning web scraping, data extraction, and data handling.

---

## ğŸ“Œ Project Overview

This project demonstrates how to scrape data from Wikipedia using a clean and modular workflow.
The scraper can extract:

* âœ”ï¸ Page title
* âœ”ï¸ First paragraph / Summary
* âœ”ï¸ Infobox details (if available)
* âœ”ï¸ Tables (converted to Pandas DataFrames)
* âœ”ï¸ Internal links
* âœ”ï¸ References (optional)

All extracted data is displayed inside the notebook and can be exported to CSV or JSON.

---

## ğŸ› ï¸ Tech Stack

### **Languages & Tools**

* Python
* Jupyter Notebook

### **Libraries Used**

* `requests` â€“ Fetching Wikipedia page HTML
* `beautifulsoup4` â€“ Parsing HTML content
* `pandas` â€“ Storing & cleaning table data
* `re` â€“ Regex for text cleaning (if used)

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ Wikipedia-Scraper
â”œâ”€â”€ ğŸ“„ Wikipedia Scraping.ipynb
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ infobox_data.csv
â”‚   â”œâ”€â”€ table_data.csv
â”‚   â””â”€â”€ summary.txt
â””â”€â”€ ğŸ“ screenshots (optional)
```

---

## â–¶ï¸ How to Run the Project

### **1. Clone the Repository**

```bash
git clone https://github.com/your-username/wikipedia-scraper.git
cd wikipedia-scraper
```

### **2. Install Required Libraries**

```bash
pip install requests beautifulsoup4 pandas
```

### **3. Launch the Notebook**

```bash
jupyter notebook
```

Open the file: **Wikipedia Scraping.ipynb**

---

## ğŸ§© Features in the Notebook

### ğŸ”¹ **1. Fetch HTML Content**

* Uses `requests.get()` to fetch Wikipedia page HTML.
* Error handling for failed connections.

### ğŸ”¹ **2. Extract Page Title & Summary**

* Pulls the main heading (`<h1>` tag)
* Extracts the lead paragraph of the article

### ğŸ”¹ **3. Scrape Infobox**

* Identifies `.infobox` table
* Converts rows into structured keyâ€“value pairs
* Saves results to CSV

### ğŸ”¹ **4. Scrape HTML Tables**

* Auto-detects all `<table>` elements
* Converts tables â†’ DataFrames
* Saves to CSV

### ğŸ”¹ **5. Extract All Internal Links**

* Collects all `/wiki/...` hyperlinks
* Filters out irrelevant or repeated links

### ğŸ”¹ **6. Save & Export**

* CSV export
* JSON export (optional)
* Display inside notebook

---

## ğŸ“Š Example Output (CSV Preview)

```
key,value
"Born", "12 January 1990"
"Occupation", "Scientist"
"Nationality", "Indian"
```

---

## ğŸš€ Possible Future Improvements

* Add scraping for categories
* Add summary NLP processing
* Add automated multi-page scraping
* Build a GUI or Streamlit interface

---

## ğŸ¤ Contributing

Contributions, improvements, and suggestions are always welcome!
Feel free to open a pull request.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## â­ Support

If this project helped you, please give it a **star â­ on GitHub**!
